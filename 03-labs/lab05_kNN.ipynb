{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbors (kNN)\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "This notebook aims use nearest neighbors (kNN) model in the classification problem. In this case, we will use the Endometrium vs. Uterus cancer dataset and the goal is to separate **endometrium** tumors from the **uterine** ones.\n",
    "\n",
    "\n",
    "### Learning objective\n",
    "\n",
    "After finished this notebook, you should be able to explain **nearest neighbors (kNN) model** and its implementation by scikit-learn implementation\n",
    "\n",
    "\n",
    "### Requirements\n",
    "\n",
    "1. This notebook was created using\n",
    "  * python 3.7.1\n",
    "  * numpy 1.15.4\n",
    "  * matplotlib 3.0.2\n",
    "  * scikit-learn 0.20.1\n",
    "\n",
    "2. You can check your Python version by running\n",
    "```python\n",
    "import sys\n",
    "print(sys.version)\n",
    "```\n",
    "\n",
    "3. and the version of any module by running\n",
    "```python\n",
    "import <module name>\n",
    "print(<module name>.__version__)\n",
    "```\n",
    "\n",
    "We will focus successively on decision trees, bagging trees and random forests.\n",
    "\n",
    "the to introduce tree-based methods on classification problems.\n",
    "\n",
    "\n",
    "We will implement our own nearest neighbors classifier and compare the results with the in-built classifiers offered by scikit-learn. \n",
    "\n",
    "Let us start by setting up our environment, loading the data, and setting up our cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the https://esmeml.github.io/data/endometrium_uterus_tumor.csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "endometrium_data = pd.read_csv('https://esmeml.github.io/data/endometrium_uterus_tumor.csv', sep=\",\")\n",
    "endometrium_data.head(n=5)\n",
    "\n",
    "X = endometrium_data.drop(['ID_REF', 'Tissue'], axis=1).values\n",
    "y = pd.get_dummies(endometrium_data['Tissue']).values[:,1]\n",
    "\n",
    "print(X.shape)\n",
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load crossvalidation.py\n",
    "def cross_validate_clf(design_matrix, labels, classifier, cv_folds):\n",
    "    \"\"\" Perform a cross-validation and returns the predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    design_matrix: (n_samples, n_features) np.array\n",
    "        Design matrix for the experiment.\n",
    "    labels: (n_samples, ) np.array\n",
    "        Vector of labels.\n",
    "    classifier:  sklearn classifier object\n",
    "        Classifier instance; must have the following methods:\n",
    "        - fit(X, y) to train the classifier on the data X, y\n",
    "        - predict_proba(X) to apply the trained classifier to the data X and return probability estimates \n",
    "    cv_folds: sklearn cross-validation object\n",
    "        Cross-validation iterator.\n",
    "        \n",
    "    Return:\n",
    "    -------\n",
    "    pred: (n_samples, ) np.array\n",
    "        Vectors of predictions (same order as labels).\n",
    "    \"\"\"\n",
    "    pred = np.zeros(labels.shape)\n",
    "    for tr, te in cv_folds:\n",
    "        classifier.fit(design_matrix[tr,:], labels[tr])\n",
    "        pos_idx = list(classifier.classes_).index(1)\n",
    "        pred[te] = (classifier.predict_proba(design_matrix[te,:]))[:, pos_idx]\n",
    "    return pred\n",
    "\n",
    "def cross_validate_clf_optimize(design_matrix, labels, classifier, cv_folds):\n",
    "    \"\"\" Perform a cross-validation and returns the predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    design_matrix: (n_samples, n_features) np.array\n",
    "        Design matrix for the experiment.\n",
    "    labels: (n_samples, ) np.array\n",
    "        Vector of labels.\n",
    "    classifier:  sklearn classifier object\n",
    "        Classifier instance; must have the following methods:\n",
    "        - fit(X, y) to train the classifier on the data X, y\n",
    "        - predict_proba(X) to apply the trained classifier to the data X and return probability estimates \n",
    "    cv_folds: sklearn cross-validation object\n",
    "        Cross-validation iterator.\n",
    "        \n",
    "    Return:\n",
    "    -------\n",
    "    pred: (n_samples, ) np.array\n",
    "        Vectors of predictions (same order as labels).\n",
    "    \"\"\"\n",
    "    pred = np.zeros(labels.shape)\n",
    "    for tr, te in cv_folds:\n",
    "        classifier.fit(design_matrix[tr,:], labels[tr])\n",
    "        print(classifier.best_params_)\n",
    "        pos_idx = list(classifier.best_estimator_.classes_).index(1)\n",
    "        pred[te] = (classifier.predict_proba(design_matrix[te,:]))[:, pos_idx]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "def stratifiedMFolds(y, num_folds):\n",
    "    kf = model_selection.StratifiedKFold(n_splits=num_folds)\n",
    "    folds_regr = [(tr, te) for (tr, te) in kf.split(np.zeros(y.size), y)]\n",
    "    return folds_regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create 10 cross validate folds on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_folds = stratifiedMFolds(y, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the previously written cross validation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's redefine the cross-validation procedure with standardization\n",
    "from sklearn import preprocessing\n",
    "def cross_validate(design_matrix, labels, regressor, cv_folds):\n",
    "    \"\"\" Perform a cross-validation and returns the predictions. \n",
    "    Use a scaler to scale the features to mean 0, standard deviation 1.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    design_matrix: (n_samples, n_features) np.array\n",
    "        Design matrix for the experiment.\n",
    "    labels: (n_samples, ) np.array\n",
    "        Vector of labels.\n",
    "    classifier:  Regressor instance; must have the following methods:\n",
    "        - fit(X, y) to train the regressor on the data X, y\n",
    "        - predict_proba(X) to apply the trained regressor to the data X and return predicted values\n",
    "    cv_folds: sklearn cross-validation object\n",
    "        Cross-validation iterator.\n",
    "        \n",
    "    Return:\n",
    "    -------\n",
    "    pred: (n_samples, ) np.array\n",
    "        Vectors of predictions (same order as labels).\n",
    "    \"\"\"\n",
    "    \n",
    "    n_classes = np.unique(labels).size\n",
    "    pred = np.zeros((labels.shape[0], n_classes))\n",
    "    for tr, te in cv_folds:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        Xtr = scaler.fit_transform(design_matrix[tr,:])\n",
    "        ytr =  labels[tr]\n",
    "        Xte = scaler.transform(design_matrix[te,:])\n",
    "        regressor.fit(Xtr, ytr)\n",
    "        pred[te, :] = regressor.predict_proba(Xte)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. *k*-Nearest Neighbours Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A k-neighbours classifier can be initialised as `knn_clf = sklearn.neighbors.KNeighborsClassifier(n_neighbors=k)`\n",
    "\n",
    "Cross validate 20 *k*-NN classifiers on the loaded datset using `cross_validate`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn import metrics\n",
    "\n",
    "aurocs_clf = []\n",
    "# Create a range of values of k. We will use this throughout this notebook.\n",
    "k_range    = range(1,40,2) \n",
    "\n",
    "for k in k_range:\n",
    "    clf_knn    = # TODO\n",
    "    y_pred = cross_validate(X, y, clf_knn, cv_folds)\n",
    "    \n",
    "    fpr, tpr, thresholdss = metrics.roc_curve(y, y_pred[:,1])\n",
    "    aurocs_clf.append(metrics.auc(fpr,tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the AUC as a function of the number of nearest neighbours chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_range, aurocs_clf, color='blue')\n",
    "plt.xlabel('Number of nearest neighbours')\n",
    "plt.ylabel('Cross-validated AUC')\n",
    "plt.title('Nearest neighbours classification - cross validated AUC.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Find the best value for the parameter `n_neighbors` by finding the one that gives the maximum value of AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = np.argmax(aurocs_clf)\n",
    "print(k_range[best_k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now use `sklearn.model_selection.GridSearchCV` do to the same. The parameter to be cross-validated is the number of nearest neighbours to choose. Use an appropriate list to feed to `GridSearchCV` to find the best value for the nearest neighbours parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "classifier = neighbors.KNeighborsClassifier()\n",
    "\n",
    "param_grid = {'n_neighbors': k_range}\n",
    "clf_knn_opt = #TODO\n",
    "clf_knn_opt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (clf_knn_opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even when we use the same scoring strategy, the optimum is different due to the cross-validation strategy implemented by scikit-learn, which computs one AUC per fold and averages them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try choosing different scoring metrics for GridSearchCV, and see how the result changes. You can find scoring metrics [here](http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare the performance of the *k*-nearest neighbours classifier with logistic regularisation (both, non-regularised, and regularised)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "clf_logreg_l2 = linear_model.LogisticRegression()\n",
    "logreg_params = {'C': np.logspace(-3, 3, 7), \n",
    "                 'penalty': ['l2'],\n",
    "                 'solver': ['liblinear']}\n",
    "                \n",
    "clf_logreg_opt = model_selection.GridSearchCV(clf_logreg_l2, \n",
    "                                              logreg_params, \n",
    "                                              scoring='roc_auc',\n",
    "                                              cv=3,\n",
    "                                              iid=True\n",
    "                                             )\n",
    "clf_logreg_opt.fit(X, y)\n",
    "ypred_clf_logreg_opt = cross_validate(X, y, clf_logreg_opt.best_estimator_, cv_folds)\n",
    "fpr_clf_logreg_opt, tpr_clf_logreg_opt, thresh = metrics.roc_curve(y, ypred_clf_logreg_opt[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_logreg = linear_model.LogisticRegression(C=1e12,solver='liblinear')\n",
    "ypred_clf_logreg = cross_validate(X, y, clf_logreg, cv_folds)\n",
    "fpr_clf_logreg, tpr_clf_logreg, thresh = metrics.roc_curve(y, ypred_clf_logreg[:,1], pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_clf_knn_opt = # TODO\n",
    "fpr_clf_knn_opt, tpr_clf_knn_opt, thresh =  #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_l2_h,  = plt.plot(fpr_clf_logreg_opt, tpr_clf_logreg_opt, 'b-')\n",
    "logreg_h,     = plt.plot(fpr_clf_logreg, tpr_clf_logreg, 'g-')\n",
    "knn_h,        = plt.plot(fpr_clf_knn_opt, tpr_clf_knn_opt, 'r-')\n",
    "logreg_l2_auc = metrics.auc(fpr_clf_logreg_opt, tpr_clf_logreg_opt)\n",
    "logreg_auc    = metrics.auc(fpr_clf_logreg, tpr_clf_logreg)\n",
    "knn_auc       = metrics.auc(fpr_clf_knn_opt, tpr_clf_knn_opt)\n",
    "\n",
    "\n",
    "logreg_legend    = 'LogisticRegression. AUC=%.2f' %(logreg_auc)\n",
    "logreg_l2_legend = 'Regularised LogisticRegression. AUC=%.2f' %(logreg_l2_auc)\n",
    "knn_legend       = 'KNeighborsClassifier. AUC=%.2f' %(knn_auc)\n",
    "plt.legend([logreg_h, logreg_l2_h, knn_h], [logreg_legend, logreg_l2_legend, knn_legend])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves comparison for logistic regression and k-nearest neighbours classifier.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN performs worst than the logistic regression, which is expected due to the large number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting the distance measure**. You will notice that *k*-nearest neighbours classifiers measure distances between points to determine similarity. By default, we use the Euclidean distance metric. Often, using other distance metrics can prove to be helpful. Try to change the distance metric used here by passing it as an argument to the declaration of the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {}            \n",
    "y_preds     = {}\n",
    "\n",
    "# Fix a set of distance metrics to use\n",
    "d_metrics = ['euclidean', 'cityblock', 'correlation' , 'cosine']\n",
    "aurocs    = {}      \n",
    "\n",
    "for m in d_metrics:\n",
    "    aurocs[m] = []          \n",
    "    for k in k_range: \n",
    "        classifiers[m] = neighbors.KNeighborsClassifier(n_neighbors=k, metric=m) # TODO. Initialise a kNN classifier with n_neighbors=k and metric=m\n",
    "        y_preds[m]     = cross_validate(X, y, classifiers[m], cv_folds)# TODO. Cross validate on the data. Use cross_validate\n",
    "    \n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y, y_preds[m][:,1])# TODO.\n",
    "        auc                  = metrics.auc(fpr, tpr)# TODO.\n",
    "        aurocs[m].append(auc)     \n",
    "        \n",
    "        print('Metric = %-12s | k = %3d | AUC = %.3f.' %(m, k, aurocs[m][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot ROC curves for all the metrics together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "handles = [[] for i in range(len(d_metrics))]\n",
    "colors = ['blue', 'red', 'black', 'yellow'] \n",
    "\n",
    "for i in range(len(d_metrics)):              \n",
    "    handles[i], = plt.plot(k_range, aurocs[d_metrics[i]], color=colors[i])\n",
    "\n",
    "plt.xlabel('Number of nearest neighbors', fontsize=16)\n",
    "plt.ylabel('Cross-validated AUC', fontsize=16)\n",
    "plt.title('Nearest neighbors classification', fontsize=16)\n",
    "plt.legend(handles, d_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
