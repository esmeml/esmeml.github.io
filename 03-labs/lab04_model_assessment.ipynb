{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "This notebook introduces the concept of model assessment of supervised machine learning models.\n",
    "\n",
    "### Learning objective\n",
    "\n",
    "After finished this notebook, you should be able to design experiments to select and evaluate supervised machine learning models. The main concepts include:\n",
    "\n",
    "- training and testing sets\n",
    "- cross-validation\n",
    "- bootstrap\n",
    "- assess the performance of classifiers\n",
    "\n",
    "\n",
    "### Requirements\n",
    "\n",
    "1. This notebook was created using\n",
    "  * python 3.7.1\n",
    "  * numpy 1.15.4\n",
    "  * matplotlib 3.0.2\n",
    "  * scikit-learn 0.20.1\n",
    "  * seaborn 0.9.0\n",
    "  * scipy 1.1.0\n",
    "\n",
    "You can check your Python version by running\n",
    "```python\n",
    "import sys\n",
    "print(sys.version)\n",
    "```\n",
    "\n",
    "and the version of any module by running\n",
    "```python\n",
    "import <module name>\n",
    "print(<module name>.__version__)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `matplotlib` for data visualization together with its built-in interface called `pyplot`. Please check https://matplotlib.org/contents.html for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wisconsin Prognostic Breast Cancer dataset\n",
    "\n",
    "The data are available at https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data\n",
    "\n",
    "### Data description\n",
    "\n",
    "* The dataset contains 569 samples of **malign** and **benign** tumor cells\n",
    "* It has 35 columns, where the first one stores the **unique ID** number of the sample, the second one the diagnosis (M=malignant, B=benign), 3--32 ten real-valued features are computed for each cell nucleus\n",
    "\n",
    "    * radius (mean of distances from center to points on the perimeter)\n",
    "    * texture (standard deviation of gray-scale values)\n",
    "    * perimeter\n",
    "    * area\n",
    "    * smoothness (local variation in radius lengths)\n",
    "    * compactness ($\\frac{perimeter^2}{area} - 1.0$)\n",
    "    * concavity (severity of concave portions of the contour)\n",
    "    * concave points (number of concave portions of the contour)\n",
    "    * symmetry\n",
    "    * fractal dimension (coastline approximation - 1); 5 here 3--32 are divided into three parts first is Mean (3-13), Stranded Error(13-23) and Worst(23-32) and each contain 10 parameter (radius, texture,area, perimeter, smoothness,compactness,concavity,concave points,symmetry and fractal dimension).\n",
    "    \n",
    "The goal is to classify whether the breast cancer is **benign** or **malignant**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer_ds = # TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_ds.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the name of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_ds.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing the classes of the tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_ds.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the number of samples for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{n: v for n, v in zip(cancer_ds.target_names, np.bincount(cancer_ds.target))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer_ds.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the description, **0 represents malign turmor**, and **1 benign** tumor, and there are 212 benign examples. Thus, we can check to ensure that our mapping is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Distribution: 212 - Malignant, 357 - Benign\n",
    "# TODO (confirm that 0 represents malign tumor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the scikit-learn dataset type to a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df['diagnosis'] = cancer_ds.target\n",
    "# cancer_df['diagnosis'] = cancer_df['diagnosis'].map({0:'M', 1:'B'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df['diagnosis'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use histograms to check how each feature can help on classifying malign and benign tumors. In this case, \n",
    "if the two histograms are separated based on the feature, then we can say that the feature is important to discern each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10,3, figsize=(12, 9))\n",
    "\n",
    "# filter the data based on the type of diagnosis (0 = malignant, 1 = benign)\n",
    "malignant = cancer_ds.data[cancer_ds.target==0]\n",
    "benign = cancer_ds.data[cancer_ds.target==1]\n",
    "\n",
    "ax = axes.ravel()\n",
    "\n",
    "for i in range(30):\n",
    "    _,bins = np.histogram(cancer_ds.data[:,i], bins=40)\n",
    "    ax[i].hist(malignant[:,i], bins=bins, color='r', alpha=.5)\n",
    "    ax[i].hist(benign[:,i], bins=bins, color='g', alpha=0.3)\n",
    "    ax[i].set_title(cancer_ds.feature_names[i],fontsize=9)\n",
    "    ax[i].axes.get_xaxis().set_visible(False)\n",
    "    ax[i].set_yticks(())\n",
    "\n",
    "ax[0].legend(['malignant','benign'], loc='best',fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that **mean fractal dimension** (4 row, 1 column) isn't verfy useful for discerning _malign_ from _benign_ tumor. On the other hand, **worst radius**, **worst perimeter**, and **worst concave points** are important features that can give us strong hints about the type of the tumor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplot(1,3,1)\n",
    "plt.subplots(1,3, figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(cancer_df['worst symmetry'], cancer_df['worst texture'], s=cancer_df['worst area']*0.05, color='magenta', label='check', alpha=0.3)\n",
    "plt.xlabel('Worst Symmetry',fontsize=12)\n",
    "plt.ylabel('Worst Texture',fontsize=12)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(cancer_df['mean radius'], cancer_df['mean concave points'], s=cancer_df['mean area']*0.05, color='purple', label='check', alpha=0.3)\n",
    "plt.xlabel('Mean Radius',fontsize=12)\n",
    "plt.ylabel('Mean Concave Points',fontsize=12)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(cancer_df['mean radius'], cancer_df['worst perimeter'], s=cancer_df['mean area']*0.05, color='purple', label='check', alpha=0.3)\n",
    "plt.xlabel('Mean Radius',fontsize=12)\n",
    "plt.ylabel('Worst perimeter',fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension reduction through Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition, preprocessing\n",
    "\n",
    "std_scale = preprocessing.StandardScaler().fit(cancer_ds.data)\n",
    "X_scaled = std_scale.transform(cancer_ds.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO reduce to two principal components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the two principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = np.var(X_projected, axis=0)\n",
    "explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
    "print (explained_variance_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a pandas DataFrame from reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots(figsize=(7,5))\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "markers = {0:'*',1:'o'}\n",
    "alphas  = {0:.3, 1:.5}\n",
    "labels ={0:'Malignant',1:'Benign'}\n",
    "\n",
    "targets = cancer_pca_df['diagnosis'].unique()\n",
    "colors = ['r', 'g']\n",
    "\n",
    "for target, color in zip(targets, colors):\n",
    "    ix = cancer_df['diagnosis'] == target\n",
    "    ax.scatter(cancer_pca_df.loc[ix, 'pc1'], cancer_pca_df.loc[ix, 'pc2'], c=color, s=50, \n",
    "              label=labels[target], marker=markers[target], alpha=alphas[target])    \n",
    "    \n",
    "ax.legend(labels)\n",
    "ax.set_xlabel('First Principal Component', fontsize = 14)\n",
    "ax.set_ylabel('Second Principal Component', fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(pca.components_, cmap='viridis')\n",
    "plt.yticks([0,1],['1st Comp','2nd Comp'], fontsize=10)\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(cancer_ds.feature_names)),cancer_ds.feature_names,rotation=65,ha='left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_worst = list(cancer_df.columns[20:31])\n",
    "s = sns.heatmap(cancer_df[feature_worst].corr(),cmap='coolwarm') \n",
    "s.set_yticklabels(s.get_yticklabels(),rotation=30,fontsize=7)\n",
    "s.set_xticklabels(s.get_xticklabels(),rotation=30,fontsize=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification model: Logistic Regression \n",
    "\n",
    "Logistic regression is a classification model that is simple to implement but that performs very well on linearly separable classes. It is one of the most used algorithm for classification in the industry. \n",
    "\n",
    "We can understand the logistic regression as a probabailistic model, **odds ratio**, which is the odds in favor of a particular event. The odds ratio can be written as $\\frac{p}{1-p}$, where $p$ stands for the probability of the positive event. Positive in this case does not necessarily mean _good_, but it refers to the event that we want to predict, for example, the probability that a patient has a certain disease, or that a email is a spam or a ham; we can see the positive event as the class label $y = 1$. We can then, define the **logit** function as: $$logit(p) = log\\frac{1}{1-p}$$. \n",
    "\n",
    "The logit function can takes input values in the range 0 to 1 and transforms them to values over the entire real $\\mathbb{R}$ set range, which we can use to express as a linear relationship between features values and the log-odds:\n",
    "\n",
    "$$logit(p(y=1|x)) = w_ox_o + w_1x_1 + \\ldots{} + w_mx_m = \\sum\\limits_{i=1}^{m}w_ix_i = w^{T}x$$\n",
    "\n",
    "where, $p(y=1|x)$ is the conditional probability that a particular sample belongs to the class $1$ given its features $x$. \n",
    "\n",
    "Thus, we can apply the _sigmoid_ function to predict that a certain sample belongs to a specific class\n",
    "\n",
    "$$\\phi(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "where, $z$ is the linear combination of the weights and sample features and it can be calculated as $z = w^Tx = w_o + w_1x_1+\\ldots+w_mx_m$\n",
    "\n",
    "Under the assumption of linear boundaries segragating classes, the posterior probability of class $C_1$ can be written as a logistic sigmoid acting on a linear function of the feature vector X so that $\\mathbb{P}(C_1|X) = \\sigma(\\beta^TX)$.\n",
    "\n",
    "We can check plotting the sigmod function for some values in the range between -5 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmod(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "z = np.arange(-5, 5, 0.1)\n",
    "phi_z = sigmod(z)\n",
    "\n",
    "plt.plot(z, phi_z)\n",
    "plt.axvline(0.0, color='k')\n",
    "plt.axhspan(0.0, 1.0, facecolor='1.0', alpha=1.0, ls='dotted')\n",
    "plt.axhline(y = 0.5, ls='dotted', alpha=1.0, color='k')\n",
    "plt.yticks([0.0, 0.5, 1.0])\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('$\\phi(z)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $\\phi(z)$ approaches 1, $z$ goes towards infinity $(z \\mapsto \\infty)$, since $e^{-z}$ becomes very small for large values of $z$. Similarly, when $\\phi(z)$ goes towards $0$ for $z\\mapsto -\\infty$ as a result of an increasing large denominator. As a result, we can conclude that the _sigmoid_ function takes real number values as input and transforms them to values in the range [0, 1] with a intercept at $\\phi(z) = 0.5$\n",
    "\n",
    "Clearly, the output of the _sigmoid_ function is then intercept as the probability of a particular sample belongs to class 1 $\\phi(z) = \\mathbb{P}(y=1|x; w)$, given its features $x$ parametrized by the weights $x$. The probability can then be simply converted into a binary outcome via an unit step function\n",
    "\n",
    "$$\\hat{y} =\n",
    "  \\begin{cases}\n",
    "    1 & \\text{if $\\phi(z) \\geq 0.5$} \\\\    \n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scikit-learn implementation of LogicRegression\n",
    "\n",
    "scikit-learn implements Logistic Regression through the class [linear_model.LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO import the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer_ds.data, cancer_ds.target, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=1e7, solver='liblinear')\n",
    "lrm = lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score: {:.3f}\".format(lrm.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(lrm.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the predition of the model using the reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cancer_pca_df.drop(['diagnosis'], axis = 1).values\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X, cancer_pca_df.diagnosis, random_state=1)\n",
    "\n",
    "lrm_pca = LogisticRegression(C=1e10, solver='liblinear').fit(X_train_pca, y_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score: {:.3f}\".format(lrm_pca.score(X_train_pca, y_train_pca)))\n",
    "print(\"Test set score: {:.3f}\".format(lrm_pca.score(X_test_pca, y_test_pca)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import cross_validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = StratifiedKFold(n_splits=10, random_state=1)\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "scores = []\n",
    "i = 0\n",
    "for (train, test) in kfold.split(X_train, y_train):\n",
    "    lrm.fit(X_train[train], y_train[train])\n",
    "    score = lrm.score(X_train[test], y_train[test])\n",
    "    scores.append(score)\n",
    "    print('Fold %s, Class dist.: %s, Acc. %.3f' %(i + 1, np.bincount(y_train[train]), score))\n",
    "    i  +=1\n",
    "\n",
    "print('CV accuracy: %.3f +/- %.3f' %(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import cross_validation\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "scores = []\n",
    "i = 0\n",
    "for (train, test) in kf.split(X_train):\n",
    "    lrm.fit(X_train[train], y_train[train])\n",
    "    score = lrm.score(X_train[test], y_train[test])\n",
    "    scores.append(score)\n",
    "    print('Fold %s, Class dist.: %s, Acc. %.3f' %(i + 1, np.bincount(y_train[train]), score))\n",
    "    i+=1\n",
    "\n",
    "print('CV accuracy: %.3f +/- %.3f' %(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn also implements a k-fold cross-validation scorer through `cross_val_score`, which enables us to evaluate a model using stratified k-fold cross-validation more efficiently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(estimator = lrm, \n",
    "                         X=X_train_pca, \n",
    "                         y=y_train_pca, \n",
    "                         cv=10, \n",
    "                        n_jobs=1)\n",
    "print('Cross-validation accuracy scores: %s' %(scores))\n",
    "print('CV accuracy: %.3f +/- %.3f' %(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the different performance evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = lrm.predict(X_test)\n",
    "conf_matrix = #TODO\n",
    "\n",
    "print('Confusion matrix \\n{}'.format(conf_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formating the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i, s=conf_matrix[i,j], va='center', ha='center')\n",
    "\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing precision, recall, and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('F1-score: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "\n",
    "X_train2 = X_train[:, [4,14]]\n",
    "\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0,1,100)\n",
    "all_tpr = []\n",
    "\n",
    "i = 0\n",
    "for (train, test) in kfold.split(X_train, y_train):\n",
    "    probas = lrm.fit(X_train2[train], y_train[train]).predict_proba(X_train2[test])\n",
    "    fpr, tpr, thresholds = roc_curve(y_train[test], probas[:, 1], pos_label=1)\n",
    "    mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr[0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)    \n",
    "    plt.plot(fpr, tpr, lw=1, label='ROC Fold %d(area = %0.2f)' % (i + 1, roc_auc))\n",
    "    i +=1    \n",
    "    \n",
    "plt.plot([0,1], [0,1], linestyle='--', color=(0.6, 0.6, 0.6), label='random guessing')\n",
    "mean_tpr /= kfold.n_splits\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, 'k--', label='mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "plt.plot([0,0,1], \n",
    "         [0,1,1], \n",
    "         lw=2, \n",
    "        linestyle=':',\n",
    "        color='black',\n",
    "        label='perfect performance')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "plt.title('Receiver Operator Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining transformers and estimators in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline_lr = Pipeline([('scl', StandardScaler()), \n",
    "                        ('pca', PCA(n_components=2)),\n",
    "                        ('clf', LogisticRegression(C=1e7, solver='liblinear', random_state=1))])\n",
    "pipeline_lr.fit(X_train, y_train)\n",
    "print('Test accuracy: %.3f' %pipeline_lr.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
